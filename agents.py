# agents.py
# Defines the AgentState and the Agent classes (DescriptionAgent, FeatureEngAgent).

import operator
import uuid
from typing import Dict, Any
from typing import TypedDict, Annotated, Any, List
import pandas as pd
import pickle # For MemorySaver serialization if needed

# Langchain and Langgraph imports
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage
from langchain_google_genai import ChatGoogleGenerativeAI # Or your specific LLM
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver # Or SqliteSaver etc.
from langgraph.types import interrupt, Command

# Import utility functions (assuming utils.py is in the same directory or accessible)
try:
    from utils import build_data_context_from_df, ask_about_context
except ImportError:
    print("Warning: Could not import from utils.py. Ensure it's in the Python path.")
    # Define dummy functions if import fails, to allow agent definition
    def build_data_context_from_df(*args, **kwargs): return {'status': 'Failure', 'error_message': 'Utils not found'}
    def ask_about_context(*args, **kwargs): return "Error: Utils not found"


# Define the state structure for the graphs
class AgentState(TypedDict):
    """
    Represents the state passed between nodes in the LangGraph graphs.
    """
    cmsg: Any | None # Correction message/user feedback
    grams: Any # Context generated by build_data_context_from_df
    messages: Annotated[list[AnyMessage], operator.add] # Accumulated messages
    dataset: pd.DataFrame | None # The input dataset


# ==============================================================================
# Description Agent Class
# ==============================================================================

class DescriptionAgent:
    """
    An agent that analyzes a dataset, generates a description using an LLM,
    and allows for human correction/refinement.
    """
    def __init__(self, model: ChatGoogleGenerativeAI):
        """
        Initializes the DescriptionAgent with an LLM model and compiles the graph.

        Args:
            model: An initialized ChatGoogleGenerativeAI (or compatible) client.
        """
        if model is None:
            raise ValueError("LLM model must be provided to DescriptionAgent.")
        self.model = model

        # Build the graph
        g = StateGraph(AgentState)
        g.add_node("autograph", self.autograph) # Node for initial EDA/context building
        g.add_node("llm", self.call_llm) # Node for calling the LLM
        g.add_node("correction", self.human_correction) # Node for human feedback
        g.add_node("save_state", self.save_state) # Intermediate node to save state before check

        # Define graph edges
        g.add_edge("autograph", "llm") # After EDA, call LLM
        g.add_edge("correction", "llm") # After correction, call LLM again
        g.add_edge("llm", "save_state") # After LLM call, save state

        # Conditional edge based on human check
        g.add_conditional_edges(
            "save_state", # Source node
            self.human_check, # Function to decide the next step
            {
                "end": END, # If human_check returns "end"
                "correction": "correction" # If human_check returns "correction"
            }
        )

        g.set_entry_point("autograph") # Start with EDA

        # Compile the graph with a checkpointer (MemorySaver for simplicity)
        # Use SqliteSaver for persistence:
        # from langgraph.checkpoint.sqlite import SqliteSaver
        # memory = SqliteSaver.from_conn_string(":memory:") # Or a file path
        memory = MemorySaver()
        self.graph = g.compile(checkpointer=memory) # Use memory saver


    def autograph(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Performs initial EDA and context building.

        Args:
            state: The current agent state (must contain 'dataset').

        Returns:
            A dictionary updating the state with 'grams' and initializing 'messages'.
        """
        print("\nExecuting Autograph Node...")
        df = state.get("dataset")
        if df is None or not isinstance(df, pd.DataFrame):
             print("  Error: Dataset not found or invalid in state.")
             # Return state indicating failure maybe? Or raise error?
             # For now, return minimal state to potentially allow graph to continue/fail gracefully
             return {"grams": {'status': 'Failure', 'error_message': 'Dataset missing'}, "messages": []}

        # Call the utility function for context building
        # Pass the LLM client initialized in the agent
        ctx = build_data_context_from_df(df, self.model)

        # Initialize state keys required by the graph
        return {
            "grams": ctx, # The generated context dictionary
            "messages": [], # Start with no LLM messages in this cycle
            "cmsg": None, # No correction message initially
            # Keep dataset in state if needed by subsequent nodes implicitly
        }

    def call_llm(self, state: AgentState) -> Dict[str, list]:
        """
        Node function: Calls the LLM to generate or refine the description.

        Args:
            state: The current agent state.

        Returns:
            A dictionary updating the 'messages' list in the state.
        """
        print("\nExecuting LLM Node...")
        grams_result = state.get("grams")
        correction_msg = state.get("cmsg")

        if not isinstance(grams_result, dict) or grams_result.get('status') != 'Success':
            print("  Skipping LLM call: Context building failed or context is missing.")
            # Add an error message to the state?
            error_msg = grams_result.get('error_message', 'Context unavailable') if isinstance(grams_result, dict) else 'Context unavailable'
            return {"messages": state.get("messages", []) + [SystemMessage(content=f"LLM Call Skipped: {error_msg}")]}

        # Determine the prompt based on whether there's a correction message
        if correction_msg:
            print("  LLM called with correction message.")
            prompt = correction_msg # Use the feedback from the human
        else:
            print("  LLM called for initial description.")
            df_cols = list(state.get("dataset", pd.DataFrame()).columns) # Get columns safely
            prompt = (
                f"Analyze the dataset context (features: {df_cols}) provided in the previous step "
                f"(summaries, plots, representation) and tell me the story of the data.\n"
                "Focus on insights relevant for potential ML tasks.\n"
                "***Output***\n"
                "description: <Your synthesized data story here>"
            )

        # Use the ask_about_context utility function
        # It handles formatting the prompt correctly for Q&A/Refinement
        llm_response_content = ask_about_context(
            task1_results=grams_result,
            user_message=prompt, # Pass the constructed prompt or correction
            llm_client=self.model
        )

        # Append the LLM's response to the messages list
        # Ensure llm_response_content is treated as a string message
        new_messages = state.get("messages", []) + [HumanMessage(content=llm_response_content or "LLM did not return a response.")]

        return {"messages": new_messages}

    def save_state(self, state: AgentState) -> AgentState:
        """
        Node function: Simple pass-through to ensure state is saved by the
        checkpointer before potentially interrupting for human input.

        Args:
            state: The current agent state.

        Returns:
            The unchanged agent state.
        """
        print("\nExecuting Save State Node...")
        # No modification needed, just ensures checkpointing happens
        return state

    def human_check(self, state: AgentState) -> str:
        """
        Conditional edge function: Interrupts the graph to ask for human approval.

        Args:
            state: The current agent state.

        Returns:
            'end' if approved, 'correction' if not approved.
        """
        print("\nExecuting Human Check Node...")
        last_message = state.get("messages", [])[-1] if state.get("messages") else SystemMessage(content="No message generated yet.")

        # Display the last message for the user
        print("\n--- LLM Output ---")
        print(last_message.content) # Assuming the last message content is the description
        print("--------------------")

        # Interrupt and ask for approval (simulated here with input)
        # In a real app, this would involve a UI interaction
        while True:
            user_input = input("Do you approve the LLM's output? (yes/no): ").strip().lower()
            if user_input == "yes":
                print("  User approved. Ending graph.")
                return "end"
            elif user_input == "no":
                print("  User disapproved. Proceeding to correction.")
                return "correction"
            else:
                print("  Invalid input. Please enter 'yes' or 'no'.")


    def human_correction(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Interrupts the graph to get human feedback/correction.

        Args:
            state: The current agent state.

        Returns:
            A dictionary updating the 'cmsg' (correction message) in the state.
        """
        print("\nExecuting Human Correction Node...")
        last_llm_msg_content = state.get("messages", [])[-1].content if state.get("messages") else "No previous LLM message."

        # Interrupt to get free-form feedback (simulated with input)
        feedback = input("What aspect should be improved or what should the LLM focus on now? ")

        # Format the feedback to guide the next LLM call
        correction_prompt = (
            f"The previous description was:\n'{last_llm_msg_content}'\n\n"
            f"My feedback is: '{feedback}'.\n\nPlease generate a revised description incorporating this feedback."
            "\n***Output***\n"
            "description: <Your revised data story here>"
        )

        # Update the state with the new correction message
        return {"cmsg": correction_prompt}


    def run(self, df: pd.DataFrame, thread_id: str | None = None) -> tuple[dict, AnyMessage | None]:
        """
        Runs the DescriptionAgent graph.

        Args:
            df: The input pandas DataFrame.
            thread_id: An optional ID for conversation threading/persistence.
                       If None, a new one is generated.

        Returns:
            A tuple containing the final state dictionary and the last message generated.
            Returns (None, None) if the input DataFrame is invalid.
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            print("Error: Invalid or empty DataFrame provided.")
            return None, None

        if thread_id is None:
            thread_id = uuid.uuid4().hex
            print(f"Starting new thread: {thread_id}")
        else:
             print(f"Continuing thread: {thread_id}")

        cfg = {"configurable": {"thread_id": thread_id}}

        # Initial state only needs the dataset
        state_input = {"dataset": df}
        final_state_values = None
        last_message = None

        try:
            # Stream events and handle interruptions
            for event in self.graph.stream(state_input, config=cfg, stream_mode="values"):
                # The stream yields the full state dictionary at each step
                final_state_values = event # Keep track of the latest state
                # No explicit interruption handling needed here as human_check/correction use input()
                # In a UI, you'd check for specific interruption signals/events.
                print(".", end="", flush=True) # Show progress

            print("\nGraph execution finished.")
            # Get the final state after the loop finishes
            if final_state_values:
                last_message = final_state_values.get("messages", [])[-1] if final_state_values.get("messages") else None

        except Exception as e:
             print(f"\nError during graph execution: {e}")
             import traceback
             traceback.print_exc()
             # Try to get the state even if there was an error
             try:
                 final_state_values = self.graph.get_state(cfg).values
                 last_message = final_state_values.get("messages", [])[-1] if final_state_values.get("messages") else None
             except Exception as state_err:
                  print(f"Could not retrieve final state after error: {state_err}")
                  final_state_values = {"error": str(e)} # Store error in state if possible
                  last_message = None

        return final_state_values, last_message


# ==============================================================================
# Feature Engineering Agent Class (Placeholder/Simplified)
# ==============================================================================

class FeatureEngAgent:
    """
    An agent designed for feature engineering tasks (simplified example).
    It takes an initial description and aims to select/engineer features.
    """
    def __init__(self, model: ChatGoogleGenerativeAI):
        """
        Initializes the FeatureEngAgent.

        Args:
            model: An initialized ChatGoogleGenerativeAI (or compatible) client.
        """
        if model is None:
            raise ValueError("LLM model must be provided to FeatureEngAgent.")
        self.model = model

        # Build the graph for feature engineering
        g = StateGraph(AgentState)
        g.add_node("auto_description_refinement", self.auto_description_refinement)
        g.add_node("select_feature", self.select_feature)
        g.add_node("eng_feature", self.eng_feature)

        # Define edges (simple linear flow for this example)
        g.add_edge("auto_description_refinement", "select_feature")
        g.add_edge("select_feature", "eng_feature")
        g.add_edge("eng_feature", END) # End after engineering step

        g.set_entry_point("auto_description_refinement")

        # Use a checkpointer if state persistence is needed between runs
        memory = MemorySaver()
        self.graph = g.compile(checkpointer=memory)

    def auto_description_refinement(self, state: AgentState) -> Dict[str, list]:
        """
        Node function: Refines the initial description based on user input (if any).
        (Simplified: In the notebook, it seemed to just call the LLM again).
        """
        print("\nExecuting Auto Description Refinement Node...")
        last_msg_content = state.get("messages", [])[-1].content if state.get("messages") else "No prior description available."
        user_context_msg = state.get("cmsg") # User context provided at start

        # Construct a prompt for refinement or initial generation if needed
        if user_context_msg:
            prompt = (
                f"Based on the previous data story:\n'{last_msg_content}'\n\n"
                f"And the user's context: '{user_context_msg}'\n\n"
                "Refine the data story, focusing on aspects relevant for feature engineering. "
                "Identify key patterns or issues mentioned."
                "\n***Output***\n"
                "refined_description: <Your refined story/analysis here>"
            )
        else:
             prompt = (
                f"Review the data story:\n'{last_msg_content}'\n\n"
                "Identify key patterns, correlations, or potential issues mentioned that could be "
                "relevant for feature engineering."
                "\n***Output***\n"
                "analysis_for_fe: <Your analysis here>"
             )

        try:
            llm_response = self.model.invoke([HumanMessage(content=prompt)])
            print(f"  LLM refinement response received.")
            refined_description = llm_response.content
        except Exception as e:
            print(f"  Error during LLM refinement call: {e}")
            refined_description = f"Error in refinement: {e}"

        return {"messages": state.get("messages", []) + [HumanMessage(content=refined_description)]}


    def select_feature(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Placeholder for selecting features based on the description.
        """
        print("\nExecuting Select Feature Node...")
        df = state.get("dataset")
        description = state.get("messages", [])[-1].content if state.get("messages") else "No description."

        # TODO: Implement actual feature selection logic, possibly using LLM
        # Example prompt structure:
        prompt = (
            f"Based on this data description:\n'{description}'\n\n"
            f"And the available columns: {list(df.columns) if df is not None else 'N/A'}\n\n"
            "Suggest one or two potential feature engineering ideas (e.g., combining columns, creating interaction terms, binning). "
            "Explain your reasoning briefly."
            "\n***Output***\n"
            "suggestions: <List your suggestions and reasoning here>"
        )
        print("  (Placeholder) LLM would be called here to suggest features.")
        # llm_response = self.model.invoke([HumanMessage(content=prompt)])
        # feature_suggestion = llm_response.content
        feature_suggestion = "Placeholder: Suggest creating interaction term between 'alcohol' and 'volatile_acidity'."


        # Update state - maybe add suggestions to messages or a new state key
        return {"messages": state.get("messages", []) + [HumanMessage(content=f"Feature Suggestion: {feature_suggestion}")]}


    def eng_feature(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Placeholder for actually engineering the selected feature.
        """
        print("\nExecuting Engineer Feature Node...")
        df = state.get("dataset")
        last_message = state.get("messages", [])[-1].content if state.get("messages") else ""

        # TODO: Implement feature engineering based on the suggestion in last_message
        print(f"  (Placeholder) Feature engineering based on: '{last_message}' would happen here.")
        # Example: If suggestion was to create interaction term
        # if df is not None and 'alcohol' in df and 'volatile_acidity' in df:
        #     df['alcohol_x_vol_acidity'] = df['alcohol'] * df['volatile_acidity']
        #     print("  Engineered 'alcohol_x_vol_acidity' feature.")
        # else:
        #     print("  Could not engineer feature (missing columns or DataFrame).")

        engineered_feature_result = "Placeholder: Engineered feature 'alcohol_x_vol_acidity'."

        # Update state - potentially update the dataset in the state if modified
        return {
            "messages": state.get("messages", []) + [HumanMessage(content=engineered_feature_result)],
            "dataset": df # Return the potentially modified DataFrame
            }


    def eng(self, df: pd.DataFrame, description: str | None = None, initial_grams: dict | None = None, initial_messages: list | None = None) -> Dict[str, Any] | None:
        """
        Runs the Feature Engineering agent graph.

        Args:
            df: The input pandas DataFrame.
            description: Optional initial user context/description to refine.
            initial_grams: Optional pre-computed context from DescriptionAgent.
            initial_messages: Optional initial message list (e.g., from DescriptionAgent).

        Returns:
            The final state dictionary or None if execution fails.
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            print("Error: Invalid or empty DataFrame provided to FeatureEngAgent.")
            return None

        thread_id = uuid.uuid4().hex # Use a new thread for each FE run for simplicity
        cfg = {"configurable": {"thread_id": thread_id}}

        # Construct initial state
        state_input = {
            "dataset": df,
            "cmsg": description, # User context for refinement node
            "grams": initial_grams or {}, # Context from EDA
            "messages": initial_messages or [] # Messages from EDA agent
        }

        final_state_values = None
        print(f"\nStarting Feature Engineering Agent (Thread: {thread_id})...")
        try:
            # Stream the execution
            for event in self.graph.stream(state_input, config=cfg, stream_mode="values"):
                 final_state_values = event # Update state
                 last_msg = event.get("messages", [])[-1].content if event.get("messages") else "No message"
                 print(f"  Node executed. Last message: {last_msg[:100]}...") # Show progress

            print("\nFeature Engineering graph execution finished.")

        except Exception as e:
            print(f"\nError during Feature Engineering graph execution: {e}")
            import traceback
            traceback.print_exc()
             # Try to get the state even if there was an error
            try:
                final_state_values = self.graph.get_state(cfg).values
            except Exception as state_err:
                 print(f"Could not retrieve final state after error: {state_err}")
                 final_state_values = {"error": str(e)}

        return final_state_values

