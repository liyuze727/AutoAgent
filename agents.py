# agents.py
# Defines the AgentState and the Agent classes (DescriptionAgent, FeatureEngAgent).

import operator
import uuid
import pandas as pd
import numpy as np # Make sure numpy is imported for df.eval
import pickle # For MemorySaver serialization if needed
import traceback # For error handling in eval
import ast # For safely evaluating LLM string response
from typing import TypedDict, Annotated, Any, List, Dict, Optional

# Langchain and Langgraph imports
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_google_genai import ChatGoogleGenerativeAI # Or your specific LLM
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver # Or SqliteSaver etc.
from langgraph.types import interrupt, Command
from IPython.display import display # For potential preview in eng()

# Import utility functions (assuming utils.py is in the same directory or accessible)
try:
    from utils import build_data_context_from_df, ask_about_context
except ImportError:
    print("Warning: Could not import from utils.py. Ensure it's in the Python path.")
    # Define dummy functions if import fails, to allow agent definition
    def build_data_context_from_df(*args, **kwargs): return {'status': 'Failure', 'error_message': 'Utils not found'}
    def ask_about_context(*args, **kwargs): return "Error: Utils not found"


# Define the state structure for the graphs
class AgentState(TypedDict):
    """
    Represents the state passed between nodes in the LangGraph graphs.
    """
    cmsg: Optional[str] # Correction message/user feedback/context for FE
    grams: Dict[str, Any] # Context generated by build_data_context_from_df
    messages: Annotated[List[AnyMessage], operator.add] # Accumulated messages
    dataset: pd.DataFrame # The input dataset (can be modified by FE)
    proposed_features: Dict[str, str] # To store {"new_feature_name": "formula_string"}
    feature_report: str # To store messages about feature creation success/failure


# ==============================================================================
# Description Agent Class (Unchanged from previous version)
# ==============================================================================

class DescriptionAgent:
    """
    An agent that analyzes a dataset, generates a description using an LLM,
    and allows for human correction/refinement.
    """
    def __init__(self, model: ChatGoogleGenerativeAI):
        """
        Initializes the DescriptionAgent with an LLM model and compiles the graph.

        Args:
            model: An initialized ChatGoogleGenerativeAI (or compatible) client.
        """
        if model is None:
            raise ValueError("LLM model must be provided to DescriptionAgent.")
        self.model = model

        # Build the graph
        g = StateGraph(AgentState)
        g.add_node("autograph", self.autograph) # Node for initial EDA/context building
        g.add_node("llm", self.call_llm) # Node for calling the LLM
        g.add_node("correction", self.human_correction) # Node for human feedback
        g.add_node("save_state", self.save_state) # Intermediate node to save state before check

        # Define graph edges
        g.add_edge("autograph", "llm") # After EDA, call LLM
        g.add_edge("correction", "llm") # After correction, call LLM again
        g.add_edge("llm", "save_state") # After LLM call, save state

        # Conditional edge based on human check
        g.add_conditional_edges(
            "save_state", # Source node
            self.human_check, # Function to decide the next step
            {
                "end": END, # If human_check returns "end"
                "correction": "correction" # If human_check returns "correction"
            }
        )

        g.set_entry_point("autograph") # Start with EDA

        # Compile the graph with a checkpointer (MemorySaver for simplicity)
        memory = MemorySaver()
        self.graph = g.compile(checkpointer=memory) # Use memory saver


    def autograph(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Performs initial EDA and context building.

        Args:
            state: The current agent state (must contain 'dataset').

        Returns:
            A dictionary updating the state with 'grams' and initializing 'messages'.
        """
        print("\nExecuting Autograph Node (Description Agent)...")
        df = state.get("dataset")
        if df is None or not isinstance(df, pd.DataFrame):
             print("  Error: Dataset not found or invalid in state.")
             return {"grams": {'status': 'Failure', 'error_message': 'Dataset missing'}, "messages": [], "dataset": None}

        # Call the utility function for context building
        ctx = build_data_context_from_df(df, self.model) # Pass LLM client

        # Initialize state keys required by the graph
        return {
            "grams": ctx, # The generated context dictionary
            "messages": [], # Start with no LLM messages in this cycle
            "cmsg": None, # No correction message initially
            "dataset": df # Pass dataset along
        }

    def call_llm(self, state: AgentState) -> Dict[str, list]:
        """
        Node function: Calls the LLM to generate or refine the description.

        Args:
            state: The current agent state.

        Returns:
            A dictionary updating the 'messages' list in the state.
        """
        print("\nExecuting LLM Node (Description Agent)...")
        grams_result = state.get("grams")
        correction_msg = state.get("cmsg")
        current_messages = state.get("messages", [])

        # Ensure grams_result is valid before proceeding
        if not isinstance(grams_result, dict) or grams_result.get('status') != 'Success':
            print("  Skipping LLM call: Context building failed or context is missing.")
            error_msg = grams_result.get('error_message', 'Context unavailable') if isinstance(grams_result, dict) else 'Context unavailable'
            # Add system message indicating skip, don't overwrite existing messages
            return {"messages": current_messages + [SystemMessage(content=f"LLM Call Skipped: {error_msg}")]}

        # Determine the prompt based on whether there's a correction message
        if correction_msg:
            print("  LLM called with correction message.")
            # Use ask_about_context for refinement based on feedback
            prompt_for_llm = correction_msg # The correction node formats this
        else:
            print("  LLM called for initial description.")
            df = state.get("dataset")
            df_cols = list(df.columns) if df is not None else ["Unknown"]
            # Use ask_about_context for initial story generation
            prompt_for_llm = (
                f"Analyze the dataset context (features: {df_cols}) provided in the 'grams' state "
                f"and tell me the story of the data.\n"
                "Focus on insights relevant for potential ML tasks.\n"
                "***Output***\n"
                "description: <Your synthesized data story here>"
            )

        # Use the ask_about_context utility function
        llm_response_content = ask_about_context(
            task1_results=grams_result,
            user_message=prompt_for_llm, # Pass the constructed prompt or correction
            llm_client=self.model
        )

        # Append the LLM's response as an AIMessage
        new_messages = current_messages + [AIMessage(content=llm_response_content or "LLM did not return a response.")]

        return {"messages": new_messages}

    def save_state(self, state: AgentState) -> AgentState:
        """
        Node function: Pass-through to save state before human check.
        """
        print("\nExecuting Save State Node (Description Agent)...")
        return state # No modification needed

    def human_check(self, state: AgentState) -> str:
        """
        Conditional edge function: Asks for human approval via input().
        """
        print("\nExecuting Human Check Node (Description Agent)...")
        # Retrieve the latest state from the checkpointer before asking
        try:
             thread_id = state['configurable']['thread_id'] # Get thread_id if possible
             cfg = {"configurable": {"thread_id": thread_id}}
             current_state_values = self.graph.get_state(cfg).values
        except Exception:
             print("Warning: Could not retrieve latest state from checkpointer for display.")
             current_state_values = state # Fallback to passed state

        last_message = current_state_values.get("messages", [])[-1] if current_state_values.get("messages") else None

        if last_message and isinstance(last_message, AIMessage):
            print("\n--- LLM Output ---")
            print(last_message.content)
            print("--------------------")
        elif last_message:
             print("\n--- Last Message (Not from LLM) ---")
             print(last_message.content)
             print("--------------------")
        else:
            print("\nNo message generated yet.")

        # Interrupt and ask for approval
        while True:
            user_input = input("Do you approve the LLM's output? (yes/no): ").strip().lower()
            if user_input in ["yes", "y"]:
                print("  User approved. Ending graph.")
                return "end" # Corresponds to the key in add_conditional_edges
            elif user_input in ["no", "n"]:
                print("  User disapproved. Proceeding to correction.")
                return "correction" # Corresponds to the key in add_conditional_edges
            else:
                print("  Invalid input. Please enter 'yes' or 'no'.")

    def human_correction(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Gets human feedback via input().
        """
        print("\nExecuting Human Correction Node (Description Agent)...")
        last_llm_msg_content = state.get("messages", [])[-1].content if state.get("messages") else "No previous LLM message."

        # Interrupt to get free-form feedback
        feedback = input("What aspect should be improved or what should the LLM focus on now? ")

        # Format the feedback to guide the next LLM call via ask_about_context
        correction_prompt = (
             f"The previous description was:\n'{last_llm_msg_content}'\n\n"
             f"My feedback is: '{feedback}'.\n\nPlease generate a revised description incorporating this feedback."
             "\n***Output***\n"
             "description: <Your revised data story here>"
        )

        # Update the state with the new correction message for the 'llm' node
        # Also add the human feedback itself to the message history
        return {
            "cmsg": correction_prompt,
            "messages": state.get("messages", []) + [HumanMessage(content=feedback)]
            }


    def run(self, df: pd.DataFrame, thread_id: str | None = None) -> tuple[dict | None, AnyMessage | None]:
        """
        Runs the DescriptionAgent graph using input() for human interaction.

        Args:
            df: The input pandas DataFrame.
            thread_id: An optional ID for conversation threading/persistence.

        Returns:
            A tuple containing the final state dictionary and the last message generated,
            or (None, None) if input is invalid or execution fails critically.
        """
        if not isinstance(df, pd.DataFrame) or df.empty:
            print("Error: Invalid or empty DataFrame provided to DescriptionAgent.")
            return None, None

        if thread_id is None:
            thread_id = uuid.uuid4().hex
            print(f"Starting new thread for DescriptionAgent: {thread_id}")
        else:
             print(f"Continuing thread for DescriptionAgent: {thread_id}")

        cfg = {"configurable": {"thread_id": thread_id}}

        # Initial state only needs the dataset
        state_input: Any = {"dataset": df} # Start with the dict
        final_state_values = None
        last_message = None

        print("\n=== Running Description Agent ===")
        try:
            # The loop structure is handled internally by stream when using interrupts
            # that rely on standard input like this. We just process the final result.
             for event in self.graph.stream(state_input, config=cfg, stream_mode="values"):
                 # Keep track of the latest state value yielded by the stream
                 final_state_values = event
                 print(".", end="", flush=True) # Show progress

             print("\nDescription Agent graph execution finished.")

             # Get the final state after the stream finishes
             if final_state_values:
                 last_message = final_state_values.get("messages", [])[-1] if final_state_values.get("messages") else None
             else:
                 # Attempt to get state if stream ended unexpectedly
                 try:
                      final_state_values = self.graph.get_state(cfg).values
                      last_message = final_state_values.get("messages", [])[-1] if final_state_values.get("messages") else None
                 except Exception as state_err:
                      print(f"Could not retrieve final state after stream ended: {state_err}")
                      final_state_values = {"error": "Stream ended unexpectedly"}
                      last_message = None

        except Exception as e:
             print(f"\nError during Description Agent graph execution: {e}")
             traceback.print_exc()
             # Try to get the state even if there was an error
             try:
                 final_state_values = self.graph.get_state(cfg).values
                 last_message = final_state_values.get("messages", [])[-1] if final_state_values.get("messages") else None
             except Exception as state_err:
                  print(f"Could not retrieve final state after error: {state_err}")
                  final_state_values = {"error": str(e)}
                  last_message = None

        return final_state_values, last_message


# ==============================================================================
# Feature Engineering Agent Class (Updated with notebook logic)
# ==============================================================================

class FeatureEngAgent:
    """
    An agent that proposes and engineers features based on EDA context and LLM suggestions.
    """
    def __init__(self, model: ChatGoogleGenerativeAI):
        """
        Initializes the FeatureEngAgent.

        Args:
            model: An initialized ChatGoogleGenerativeAI (or compatible) client.
        """
        if model is None:
            raise ValueError("LLM model must be provided to FeatureEngAgent.")
        self.model = model

        # Build the graph for feature engineering
        g = StateGraph(AgentState)
        # Renamed node for clarity
        g.add_node("prepare_fe_state", self.prepare_fe_state)
        g.add_node("select_feature", self.select_feature)
        g.add_node("eng_feature", self.eng_feature)

        # Define edges (simple linear flow for this example)
        g.set_entry_point("prepare_fe_state")
        g.add_edge("prepare_fe_state", "select_feature")
        g.add_edge("select_feature", "eng_feature")
        g.add_edge("eng_feature", END) # End after engineering step

        # Use a checkpointer if state persistence is needed between runs
        memory = MemorySaver()
        self.graph = g.compile(checkpointer=memory)


    def prepare_fe_state(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Initializes or prepares the state specifically for the FE agent run.
        Ensures necessary keys exist and potentially refines the description if needed.
        """
        print("\nExecuting Prepare FE State Node...")
        # Ensure required keys for this agent are present
        messages = state.get("messages", [])
        grams = state.get("grams", {})
        dataset = state.get("dataset")
        cmsg = state.get("cmsg") # This might hold user context for FE

        # Get the most relevant description (could be last AI message or user context)
        description_for_fe = "No specific description provided for FE."
        if cmsg: # Prioritize user context if provided
            description_for_fe = cmsg
        elif messages:
            # Find the last relevant message (likely AI or non-yes/no Human)
            for msg in reversed(messages):
                if isinstance(msg, (AIMessage, SystemMessage)):
                    description_for_fe = msg.content
                    break
                elif isinstance(msg, HumanMessage) and msg.content.lower() not in ['y', 'n', 'yes', 'no']:
                    description_for_fe = msg.content
                    break
        print(f"  Using description for FE: {description_for_fe[:100]}...")

        # Store original columns if not already present in grams
        if 'original_columns' not in grams and isinstance(dataset, pd.DataFrame):
            grams['original_columns'] = set(dataset.columns)

        # Return the prepared state, ensuring all keys for this graph are initialized
        return {
            "messages": messages,
            "grams": grams,
            "dataset": dataset,
            "cmsg": description_for_fe, # Store the description to be used
            "proposed_features": {}, # Initialize for this run
            "feature_report": "" # Initialize for this run
        }


    def select_feature(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Uses LLM to propose new features based on description
        and existing columns, requesting formulas compatible with df.eval().
        """
        print("\nExecuting Select Feature Node...")
        df = state.get("dataset")
        grams = state.get("grams", {})
        description = state.get("cmsg", "No description available.") # Use description from prepare_fe_state
        messages = state.get("messages", [])

        if df is None or not isinstance(df, pd.DataFrame):
            print("  Error: Dataset not found or invalid in state for feature selection.")
            return {"messages": messages + [AIMessage(content="Feature proposal skipped: Dataset missing.")],
                    "proposed_features": {}}

        column_list = df.columns.tolist()
        # Use the potentially more detailed EDA summary if available
        eda_summary = grams.get('final_llm_summary', 'No automated EDA summary available.')

        # Construct the prompt for the LLM to propose features
        prompt = f"""You are a data scientist specializing in feature engineering for clustering or prediction tasks.
Based on the following dataset description, EDA summary, and existing features, propose 3-5 new potentially useful engineered features.
For each proposed feature, provide a unique, valid Python variable name (snake_case, no spaces/special chars except _) and a Python expression string using pandas/numpy operations **referencing column names directly (without df['...'])**.

**Dataset Description/Context:**
{description}

**Automated EDA Summary (if available):**
{eda_summary}

**Existing Features:**
{column_list}

**Task:**
Propose 3-5 new features. Focus on creating potentially meaningful interactions (e.g., `colA * colB`), ratios (e.g., `colA / (colB + 1e-6)`), polynomial terms (e.g., `colA**2`), or simple transformations (e.g., `log1p(colA)` for skewed data) based on the data context. Ensure the Python expressions are valid for use with pandas `df.eval()`. Use `log1p` for log transforms. Add a small epsilon (e.g., 1e-6) to denominators for safe division. Explain the reasoning for each proposed feature very briefly.

**Output Format:**
Provide ONLY a Python dictionary string where keys are the new feature names (as strings) and values are the Python expression strings **using direct column names**.
Example:
{{
  "acidity_ratio": "fixed_acidity / (volatile_acidity + 1e-6)", // Reason: Explore balance between acid types.
  "sulfur_dioxide_ratio": "free_sulfur_dioxide / (total_sulfur_dioxide + 1e-6)", // Reason: Ratio might be more informative than absolute values.
  "sugar_density_interaction": "residual_sugar * density" // Reason: Investigate combined effect.
}}
"""
        # --- Invoke LLM and Parse ---
        proposed_features_dict = {}
        llm_raw_response = "LLM Invocation Failed"
        proposal_message = "Feature proposal failed." # Default message

        try:
            print("  Invoking LLM for feature proposals...")
            llm_response = self.model.invoke([HumanMessage(content=prompt)])
            llm_raw_response = llm_response.content
            print(f"  LLM Proposal Response (raw): {llm_raw_response[:200]}...") # Print beginning of response

            # --- Parsing Logic ---
            # Attempt to find the dictionary within the response
            start_index = llm_raw_response.find('{')
            end_index = llm_raw_response.rfind('}')

            if start_index != -1 and end_index != -1 and start_index < end_index:
                dict_string = llm_raw_response[start_index : end_index + 1]
                try:
                    # Use ast.literal_eval for safer evaluation than eval()
                    proposed_features_dict = ast.literal_eval(dict_string)
                    if not isinstance(proposed_features_dict, dict):
                        raise ValueError("LLM response did not evaluate to a dictionary.")
                    # Basic validation: ensure keys and values are strings
                    if not all(isinstance(k, str) and isinstance(v, str) for k, v in proposed_features_dict.items()):
                        raise ValueError("Dictionary keys/values must be strings.")

                    proposal_message = f"LLM proposed features: {list(proposed_features_dict.keys())}"
                    print(f"  Successfully parsed features: {list(proposed_features_dict.keys())}")

                except (SyntaxError, ValueError, TypeError) as parse_err:
                    print(f"  Error parsing LLM feature proposal dictionary: {parse_err}")
                    print(f"  Attempted to parse: {dict_string}")
                    proposal_message = f"LLM proposed features but parsing failed. Error: {parse_err}"
                    proposed_features_dict = {} # Reset on failure
            else:
                print("  Could not find a dictionary structure '{...}' in the LLM response.")
                proposal_message = "LLM response did not contain a parsable dictionary."
                proposed_features_dict = {}

        except Exception as e:
            print(f"  Error invoking LLM for feature proposal: {e}")
            proposal_message = f"Error invoking LLM: {e}"
            proposed_features_dict = {}

        # Update state
        return {
            "messages": messages + [AIMessage(content=proposal_message)],
            "proposed_features": proposed_features_dict, # Store the parsed dict (or empty if failed)
            "feature_report": "" # Reset report for the next step
        }


    def eng_feature(self, state: AgentState) -> Dict[str, Any]:
        """
        Node function: Attempts to create the proposed features using df.eval()
        and adds them to the dataframe copy in the state.
        """
        print("\nExecuting Engineer Feature Node...")
        dataset = state.get("dataset")
        proposed_features = state.get("proposed_features", {})
        messages = state.get("messages", [])
        grams = state.get("grams", {}) # Get grams to access original columns if needed

        if dataset is None or not isinstance(dataset, pd.DataFrame):
             print("  Error: Dataset not found or invalid in eng_feature.")
             return {"feature_report": "Error: Dataset missing.", "messages": messages + [AIMessage(content="Feature engineering skipped: Dataset missing.")]}

        # Work on a copy to avoid modifying the original state DataFrame directly in case of errors
        temp_df = dataset.copy()
        original_columns = set(grams.get('original_columns', list(dataset.columns))) # Get original columns safely

        report_lines = []
        successfully_added = []
        failed_eval = []

        if not proposed_features:
            report_lines.append("No features were proposed in the previous step.")
            print("  No features proposed.")
        else:
            print(f"  Attempting to engineer: {list(proposed_features.keys())}")
            # Provide numpy in the evaluation context
            eval_globals = {'np': np, 'log1p': np.log1p, 'exp': np.exp, 'log': np.log}

            for name, formula in proposed_features.items():
                # Basic validation for name and formula
                if not isinstance(name, str) or not name.isidentifier() or not isinstance(formula, str) or not formula:
                    report_lines.append(f"Skipping feature '{name}': Invalid name or empty formula.")
                    failed_eval.append(str(name))
                    continue
                if name in temp_df.columns:
                    report_lines.append(f"Skipping feature '{name}': Column already exists.")
                    # Don't add to failed if it already exists
                    continue

                try:
                    print(f"  Evaluating: {name} = {formula}")
                    # Use df.eval() - requires column names to be valid identifiers
                    # Pass numpy functions via local_dict or ensure they are in globals
                    new_col_data = temp_df.eval(formula, engine='python', global_dict=eval_globals, local_dict={})

                    # Check result shape and type before adding
                    if isinstance(new_col_data, (pd.Series, np.ndarray)) and len(new_col_data) == len(temp_df):
                        temp_df[name] = new_col_data # Add the new column to the copy
                        report_lines.append(f"Successfully engineered feature: '{name}'")
                        successfully_added.append(name)
                    else:
                         report_lines.append(f"Failed feature '{name}': Eval result had incorrect shape/type ({type(new_col_data)}).")
                         failed_eval.append(name)

                except Exception as e:
                    # Provide more context on error
                    err_type = type(e).__name__
                    report_lines.append(f"Failed feature '{name}': Error - {err_type}: {e}\n  Formula: {formula}")
                    print(f"  Failed feature '{name}': {err_type} - {e}")
                    # Uncomment below for full traceback during debugging
                    # print(traceback.format_exc())
                    failed_eval.append(name)

        final_report = "Feature Engineering Report:\n" + "\n".join(report_lines)
        print(final_report)

        # Update the state with the modified DataFrame (copy) and the report
        return {
            "dataset": temp_df, # The DataFrame with potentially new features
            "feature_report": final_report,
            "messages": messages + [AIMessage(content=f"Feature engineering attempt complete. Added: {successfully_added}. Failed: {failed_eval}.")]
        }


    def eng(self, df: pd.DataFrame, description_state: dict) -> Dict[str, Any] | None:
        """
        Runs the Feature Engineering agent graph.

        Args:
            df: The initial pandas DataFrame.
            description_state: The final state dictionary from the DescriptionAgent run.

        Returns:
            The final state dictionary after feature engineering attempts, or None if setup fails.
        """
        print("\n=== Starting Feature Engineering Agent ===")
        if not isinstance(df, pd.DataFrame) or df.empty:
            print("Error: Invalid or empty DataFrame provided to FeatureEngAgent.")
            return None
        if not isinstance(description_state, dict):
             print("Error: Invalid description_state provided to FeatureEngAgent.")
             return None

        thread_id = str(uuid.uuid4()) # Use a new thread for each FE run
        cfg = {"configurable": {"thread_id": thread_id}}

        # Prepare initial state using info from the description agent's final state
        initial_grams = description_state.get("grams", {})
        # Ensure original columns are set if they weren't in the description agent's grams
        if 'original_columns' not in initial_grams:
             initial_grams['original_columns'] = set(df.columns)

        initial_state_dict = {
            "dataset": df.copy(), # Start with a fresh copy of the initial data
            "messages": description_state.get("messages", []),
            "grams": initial_grams,
            "cmsg": description_state.get("cmsg"), # Pass correction/last message if any
            "proposed_features": {}, # Initialize empty for this agent run
            "feature_report": "" # Initialize empty for this agent run
        }

        final_state_values = None
        try:
            # Stream the execution - no human interaction needed in this version
            for event in self.graph.stream(initial_state_dict, config=cfg, stream_mode="values"):
                 final_state_values = event # Keep track of the latest state
                 last_node = list(event.keys())[-1] # Get the key of the last updated node
                 print(f"  Node '{last_node}' executed.")

            print("\nFeature Engineering graph execution finished.")

        except Exception as e:
            print(f"\nError during Feature Engineering graph execution: {e}")
            traceback.print_exc()
            # Try to get the state even if there was an error
            try:
                final_state_values = self.graph.get_state(cfg).values
            except Exception as state_err:
                 print(f"Could not retrieve final state after error: {state_err}")
                 final_state_values = {"error": str(e)}

        return final_state_values

